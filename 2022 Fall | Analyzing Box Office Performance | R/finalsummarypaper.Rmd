---
title: "Final Summary Paper"
author: "Arjun Bingly, Caroline Krall, Nayaeun Kwon, Tara Thomas"
date: "`r Sys.Date()`"
output:
  prettydoc::html_pretty:
    theme: cayman
    number_sections: false
    toc: yes
    toc_depth: 6
  pdf_document:
    toc: yes
    toc_depth: '3'
---

## Topic and Smart Questions
Topic: We are once again using the IMDb, Rotten Tomatoes, and lifetime gross datasets to perform analysis. Broadly, we want to understand which movies received high ratings, and how successfully we can predict the genre of the movie.We treated the analysis of genre are a yes or no problem. Instead of predicting which movie is which genre, we asked if a particular movie was a comedy or not. We used the IMDb genre variable as there were fewer issues with null values.
SMART Questions:
1. What features affect IMDb Rating?
2. What features can be used to predict movie genre?
3. What features affect the Rotten Tomatoes freshness ratings?


```{r setup, include=FALSE}
# Setting Environment 
library(dplyr)
library(tidyr)
library(ggplot2)
library(ezids)
library(stringr)
knitr::opts_chunk$set(echo=FALSE, warning=FALSE, fig.align = 'center', error = F, message = F)
options(scipen=999)
library(purrr)
```


# Data Cleaning

```{r}
movie <- read.csv("movies.csv")
```

## Getting rid of the unnecessary columns and removing null values rows
```{r}
movie_red <-
  movie %>%
  select(X, movie_title, content_rating, genres.y, runtime, tomatometer_status, tomatometer_rating, tomatometer_count, lifetime_gross, startYear, averageRating, numVotes) %>%
  rename(genre = genres.y, imdbRating = averageRating) %>%
  na.omit()
```


### Count of genres 

Rough since this would not exactly count the unique genres as say 'Comedy' and 'comedy' would be counted seperately. 
```{r}
movie_red %>% 
   separate_rows(genre,sep=",") %>% 
   count(genre) %>%
   arrange(desc(n))
str(movie_red)
```

### Adding indicator variables for Genre

The first thing we need to do is find which genres are the most common. I created indicator variables for the top genres and summarizes their count
```{r}
movie_red <-
  movie_red %>%
  mutate(drama = ifelse(str_detect(genre, "Drama"), 1,0),
         comedy = ifelse(str_detect(genre, "Comedy"), 1,0),
         romance = ifelse(str_detect(genre, "Romance"), 1,0),
         crime = ifelse(str_detect(genre, "Crime"), 1,0),
         action = ifelse(str_detect(genre, "Action"), 1,0),
         adventure = ifelse(str_detect(genre, "Adventure"), 1,0),
         thriller = ifelse(str_detect(genre, "Thriller"), 1,0),
         documentary = ifelse(str_detect(genre, "Documentary"), 1,0),
         horror = ifelse(str_detect(genre, "Horror"), 1,0),
         biography = ifelse(str_detect(genre, "Biography"), 1,0),
         mystery = ifelse(str_detect(genre, "Mystery"), 1,0),
         scifi = ifelse(str_detect(genre, "Sci-Fi"), 1,0))
```

### Making Indicators for Tomatometer Status
```{r}
movie_red <-
  movie_red %>%
  mutate(rotten = ifelse(str_detect(tomatometer_status, "Rotten"), 1, 0),
         fresh = ifelse(str_detect(tomatometer_status, "^Fresh"), 1, 0),
         cert_fresh = ifelse(str_detect(tomatometer_status, "Certified"), 1, 0),)
```



## Genre Distribution
Note that the genres are derived from the IMDB database, since it proved to be more diverse and accurate compared to the rotten tomatoes genres. 
```{r}
movie_red$content_rating = as.factor(movie_red$content_rating)
movie_red %>% 
   separate_rows(genre,sep=",") %>% 
   count(genre) %>%
   arrange(desc(n))
```

# SMART Question 1: What features affect IMDB rating?

### Overview: IMDB Rating Variable

#### Histogram of IMDB Rating Variable
```{r results='markup'}
ggplot(movie_red, aes(x=imdbRating)) + 
  geom_histogram(color="blue", fill="blue") + labs(title="IMDB histogram plot", x="IMDB Rating", y = "Count")
```

#### Summary of IMDB Rating Variable
```{r}
library("ezids")
summaryIR <- summary(movie_red$imdbRating)
xkabledply(as.table(summaryIR), title = "Summary of IMDB Rating", wide = T)
```

```{r}
#Creating factor variables
movie_red$tomatometer_status <- as.factor(movie_red$tomatometer_status)
movie_red$content_rating <- as.factor(movie_red$content_rating)
movie_red$comedy <- as.factor(movie_red$comedy)
movie_red$action <- as.factor(movie_red$action)
movie_red$scifi <- as.factor(movie_red$scifi)
movie_red$documentary <- as.factor(movie_red$documentary)
movie_red$drama <- as.factor(movie_red$drama)
movie_red$horror <- as.factor(movie_red$horror)
movie_red$romance <- as.factor(movie_red$romance)
movie_red$crime <- as.factor(movie_red$crime)
movie_red$adventure <- as.factor(movie_red$adventure)
movie_red$mystery <- as.factor(movie_red$mystery)
movie_red$biography <- as.factor(movie_red$biography)
movie_red$thriller <- as.factor(movie_red$thriller)
```



### Feature Selection for IMDB

```{r results='markup'}
movie_red2 <-
  movie_red %>%
  select(-X, -movie_title, -genre)
library("leaps")
reg.best10 <- regsubsets(imdbRating~., data = movie_red2, nbest = 2, nvmax = 10, method = "exhaustive")  
plot(reg.best10, scale = "adjr2", main = "Adjusted R^2")
summary(reg.best10)
```

For the feature selection, both qualitative and quantitative variables were included. With the exhasutive search, the criteria of `nvmax=10` and `nbest=2` were used, which means that the plot will show a two subsets of a maximum of 10 variable models. This will ultimately assist in creating the best model. Although the qualitative variables are broken down into their respective levels, it still gives a good idea as to which variables should be run in the models. 



### Four Models based on feature selection & ANOVA


```{r}
#top numerical variables:
fit1<- lm(imdbRating~runtime+tomatometer_count+numVotes+tomatometer_rating, data=movie_red)
summary(fit1)
#top variables with genre: runtime, tomato meter count, numVotes, drama, comedy, horror, biography
fit2<-lm(imdbRating~runtime+tomatometer_count+numVotes+tomatometer_rating+drama+comedy+horror+biography, data=movie_red)
summary(fit2)
#adding qual vars- content rating and tomatometer_count
fit3<-lm(imdbRating~runtime+tomatometer_count+tomatometer_status+numVotes+tomatometer_rating+content_rating, data=movie_red)
summary(fit3)
#top variables with genre: runtime, tomato meter count, numVotes, drama, comedy, horror, biography
fit4<-lm(imdbRating~runtime+tomatometer_count+tomatometer_status+tomatometer_rating++content_rating+numVotes+drama+comedy+horror+biography, data=movie_red)
summary(fit4)
anova(fit1, fit2, fit3, fit4)
```

Four models were fit based on the adjusted $R^2$ value. The first model included the quantitative variables, the second included the significant genres, the third included the two categorical variables without genre, and the fourth included the significant quantitative, qualitative, and genre variables. 

The summary for each model shows us that each model is statistically significant with a very low p-value, much lower than .05. The ANOVA test indicates that the increases in variance between the models are significant, which also reveals very low p-values.  However, when looking at the $R^2$ values, the 2nd and 4th models account for approximately .43 of the variance, and the difference is negligible. The 2nd model uses 7 variables, whereas the 4th model uses 14. Also, with the 2nd model, all of the coefficients are statistically significant with a p-value well below .05.


# SMART Question 2: Which features can be used to predict movie genre?
# drama indicator variable into y variables
```{r}
colnames(movie_red)
```


```{r}

movieclean <- movie_red %>%
  select(content_rating, runtime, tomatometer_status, lifetime_gross, startYear, imdbRating, numVotes)
 movieclean <-
   movieclean %>%
   rename("year" = "startYear")

colnames(movieclean)
```

```{r}
movieclean$y = movie_red[,10]
movieclean$y <- as.factor(movieclean$y)
str(movieclean)
```

```{r}
library("leaps")
library("bestglm")
res.bestglm <-
  bestglm(Xy = movieclean,
          family = binomial(link = "logit"),
          IC = "AIC",
          method = "exhaustive")
res.bestglm$BestModels
dramaglm <- res.bestglm$BestModel
summary(dramaglm)
```
The best model has AIC 9827.668 with all variables, `content_rating`, `runtime`, `tomatometer_rating`, `lifeime_gross`, `year`, `imbdRating`, and `numVotes`.


```{r HosmerLemeshow3}
library("ResourceSelection") 
dramaHoslem = hoslem.test(movieclean$y, fitted(dramaglm)) # Hosmer and Lemeshow test, a chi-squared test
dramaHoslem
```

The p-value of `r dramaHoslem$p.value` is relatively high. This indicates the model is not really a good fit, despite all the coefficients are significant.

# comedy indicator variable into y variables
```{r}
movieclean$y = movie_red[,11]
movieclean$y <- as.factor(movieclean$y)
str(movieclean)
```

```{r}
res.bestglm <-
  bestglm(Xy = movieclean,
          family = binomial(link = "logit"),
          IC = "AIC",
          method = "exhaustive")
res.bestglm$BestModels
comedyglm <- res.bestglm$BestModel
summary(comedyglm)
```
The best model has AIC 9623.153 with all variables exxcept `numVotes`.

```{r HosmerLemeshow2}
comedyHoslem = hoslem.test(movieclean$y, fitted(comedyglm))
comedyHoslem
```

The p-value of `r comedyHoslem$p.value` is relatively high. This indicates the model is not really a good fit, despite all the coefficients are significant.

# romance indicator variable into y variables
```{r}
movieclean$y = movie_red[,12]
movieclean$y <- as.factor(movieclean$y)
str(movieclean)
```

```{r}
res.bestglm <-
  bestglm(Xy = movieclean,
          family = binomial(link = "logit"),
          IC = "AIC",
          method = "exhaustive")
res.bestglm$BestModels
romanceglm <- res.bestglm$BestModel
summary(romanceglm)
```
The best model has AIC 7270.020 with all variables, `content_rating`, `runtime`, `tomatometer_rating`, `lifeime_gross`, `year`, `imbdRating`, and `numVotes`.

```{r HosmerLemeshow1}
romanceHoslem = hoslem.test(movieclean$y, fitted(romanceglm))
romanceHoslem
```

The p-value of `r romanceHoslem$p.value` is relatively high. This indicates the model is not really a good fit, despite all the coefficients are significant.


# SMART Question 3: What features affect the Rotten Tomato freshness ratings?

```{r}
movie_red <-
  movie %>%
  select(X, movie_title, content_rating, genres.y, runtime, tomatometer_status, tomatometer_rating, tomatometer_count, lifetime_gross, startYear, averageRating, numVotes) %>%
  rename(genre = genres.y, imbdRating = averageRating, year=startYear) %>%
  na.omit()
```

## Adding indicator variables for each Genre
I have created indicator variables for the top 12 genres, this was not an arbitrary number. 
We arrived at the arbitrary number 12 by eliminating all genres that have a count of less than 5% of the total number of movies (~400).
```{r}
movie_red <-
  movie_red %>%
  mutate(drama = ifelse(str_detect(genre, "Drama"), 1,0),
         comedy = ifelse(str_detect(genre, "Comedy"), 1,0),
         romance = ifelse(str_detect(genre, "Romance"), 1,0),
         crime = ifelse(str_detect(genre, "Crime"), 1,0),
         action = ifelse(str_detect(genre, "Action"), 1,0),
         adventure = ifelse(str_detect(genre, "Adventure"), 1,0),
         thriller = ifelse(str_detect(genre, "Thriller"), 1,0),
         documentary = ifelse(str_detect(genre, "Documentary"), 1,0),
         horror = ifelse(str_detect(genre, "Horror"), 1,0),
         biography = ifelse(str_detect(genre, "Biography"), 1,0),
         mystery = ifelse(str_detect(genre, "Mystery"), 1,0),
         fantasy = ifelse(str_detect(genre, "Fantasy"), 1,0),
         scifi = ifelse(str_detect(genre, "Sci-Fi"), 1,0)
         )
```

```{r}
vec <- 
  movie_red %>%
    select(drama,comedy,romance,crime,action,adventure,thriller,documentary,horror,biography,mystery,fantasy,scifi) %>%
    rowSums()
nogen.count <- sum(vec == 0)
```
It can be noted that only `r nogen.count` movies are not classified in the top 12 genres, which is about `r 100*nogen.count/nrow(movie_red)`% of total movies.


## Making Indicators for Tomatoer Status
Having a look at the count of each category.
```{r}
movie_red %>% 
   separate_rows(tomatometer_status,sep=",") %>% 
   count(tomatometer_status) %>%
   arrange(desc(n))
```
```{r}
ggplot(movie_red,aes(x=factor(tomatometer_status)))+
  geom_bar(stat='count',width=0.7,fill='steelblue')+
  labs(main="Distribution of tomamatometer status",x="Tomatometer Status",y="Count")+
  geom_text(stat='count', aes(label=..count..), vjust=-0.5)+
  theme_minimal()
```
## Adding indicator variables for each status.
```{r}
movie_red <-
  movie_red %>%
  mutate(rotten = ifelse(str_detect(tomatometer_status, "Rotten"), 1, 0),
         fresh = ifelse(str_detect(tomatometer_status, "Fresh"), 1, 0),
         cert = ifelse(str_detect(tomatometer_status, "Certified"), 1, 0),)
```

## What is tomatometer status?
There are 3 Tomatometer statuses - Rotten, Fresh and Certified Fresh. 

- A movie is considered 'Rotten' when less than 60% of the reviews of the movie are positive.
- A movie is considered 'Fresh' when at least 60% of the reviews of the movie are positive.

'Certified Fresh' status is a special distinction awarded to the best-reviewed movies. In order to qualify, movies or TV shows must meet the following requirements:

- A consistent Tomatometer score of 75% or higher.
- At least five reviews from Top Critics.
- Films in wide release must have a minimum of 80 reviews. This also applies for films going from limited to wide release.
- Films in limited release must have a minimum of 40 reviews.
- Only individual seasons of a TV show are eligible, and each must have a minimum of 20 reviews.

## Is our data unbalanced?
From the first look at the bar chart, one might think that the data might be slightly unbalanced but since we are using logistic regression. We are predicting a binary outcome, which would be is the movie 'Rotten' or not. This would imply that we are consolidating the 'Fresh' and 'Certified-Fresh' groups, this makes the data quite balanced. Moreover, it is worthy to note that 'Certified-Fresh' is a special distinction given to 'Fresh' movies, hence this would not throw our model off. 

# Logistic regression model.
The aim is to build a model to predict if a movie is 'Rotten' or not. 
The predictors considered IMDB ratings and vote count, year of release, runtime, content rating, lifetime gross and genre (as shown above)

Removing variables not considered in the model.
```{r}
movie_cl <-
movie_red %>%
  select(-fresh,-cert,-tomatometer_count,-tomatometer_rating,-tomatometer_status,-X,-movie_title,-genre)
movie_cl$drama = as.factor(movie_cl$drama)
movie_cl$comedy = as.factor(movie_cl$comedy)
movie_cl$romance = as.factor(movie_cl$romance)
movie_cl$crime = as.factor(movie_cl$crime)
movie_cl$action = as.factor(movie_cl$action)
movie_cl$adventure = as.factor(movie_cl$adventure)
movie_cl$thriller = as.factor(movie_cl$thriller)
movie_cl$documentary = as.factor(movie_cl$documentary)
movie_cl$horror = as.factor(movie_cl$horror)
movie_cl$biography = as.factor(movie_cl$biography)
movie_cl$mystery = as.factor(movie_cl$mystery)
movie_cl$fantasy = as.factor(movie_cl$fantasy)
movie_cl$scifi = as.factor(movie_cl$scifi)
movie_cl$rotten = as.factor(movie_cl$rotten)
str(movie_cl)
```
## Exhaustive search
For variable selection, we are taking the brute-force method of exhaustive search, ensuring we have the best possible model. 
We are using both AIC and BIC as scoring criteria, since we expect 

- Exhaustive search with all the 19 variables, takes a long time (~10hrs). This is not surprising, since the total number of combinations possible (without interactions) is 524288 models. (Calculated below.)
```{r,include=FALSE}
library(glmulti)
```
```{r}
# Just to calculate the total number of possibilities.
glmulti.aic <- glmulti(rotten~., data=movie_cl, #model formula and data
                       level=1, #no interaction terms
                       crit=bic, #BIC evaluation criteria
                       confsetsize=128, #store the best 128 models
                       fitfunction = 'glm', #generalized linear model
                       family=binomial, #logistic
                       plotty=F, #interim plots
                       method='d') # method d -> only calculates total number of possibilities.
```
- But I had run this 10 hour long exhaustive search and results are the same as that run below. 
- I have reduced the total number of variables to 15, eliminating the genres with least count (these eliminated variables played almost no role in the results I got from the full search).

```{r}
vec <- 
  movie_red %>%
    select(drama,comedy,romance,crime,action,adventure,thriller,documentary,horror) %>%
    rowSums()
nogen.count <- sum(vec == 0)
```
- Eliminating these genres would mean that we now have, `r nogen.count` movies are not classified in genres considered, which is about `r 100*nogen.count/nrow(movie_red)`% of total movies, which is very less and should not have a huge impact on the model. 

```{r}
movie_cl <-
movie_cl %>%
  select(-scifi,-fantasy,-mystery,-biography)
str(movie_cl)
```


First lets look at how many combinations are possible.
```{r}
glmulti.aic <- glmulti(rotten~., data=movie_cl, #model formula and data
                       level=1, #no interaction terms
                       crit=bic, #BIC evaluation criteria
                       confsetsize=128, #store the best 128 models
                       fitfunction = 'glm', #generalized linear model
                       family=binomial, #logistic
                       plotty=F, #interim plots
                       method='d') 
```
We can see that just reducing the the number of variables from 19 to 15 we have reduced the number of combinations from 524288 to 32768, which is about a 93% reduction!!

```{r, cache=TRUE, echo=FALSE}
ptm <- proc.time()
glmulti.aic <- glmulti(rotten~., data=movie_cl, #model formula and data
                       level=1, #no interaction terms
                       crit=bic, #BIC evaluation criteria
                       confsetsize=128, #store the best 128 models
                       fitfunction = 'glm', #generalized linear model
                       family=binomial, #logistic
                       plotty=F) #interim plots
time1 <- proc.time() - ptm
```
This search took `time1['elapsed']/60` mins to run.

### Plots
Plot of the scoring criteria, BIC arranged from best model to worst (128 models).
The red line shows models that are less that 2 BIC units away from the best model.
```{r}
plot(glmulti.aic, type='p')
```
Plot of residuals of the 5 best models.
```{r}
plot(glmulti.aic, type='r')
```
As you can see there is no significant differences between the 5 best models.

Plot of relative importance of variables.
```{r}
plot(glmulti.aic, type='s')
```


```{r}
plot(glmulti.aic, type='w')
```
Model formulas of 5 best models.
```{r}
glmulti.aic@formulas[1:5]
```
## Model Testing
We will be testing the model by using a 10 times repeated 5 fold Cross Validation.
We will be looking at both the classification accuracy and Cohen's Kappa. 
I could not figure out how to look at AUC using the CV technique. 
```{r}
library(caret)
```
```{r}
set.seed(1023)
ctrl <- trainControl(method='repeatedcv', number=5, repeats=10)
```
```{r}
#model 1
model1 <- train(rotten ~ 1 + content_rating + imbdRating + numVotes + drama + action + documentary,
    data=movie_cl,method='glm',family='binomial',trControl=ctrl)
#model 2
model2 <- train(rotten ~ 1 + content_rating + lifetime_gross + imbdRating + numVotes + drama + action + documentary,
    data=movie_cl,method='glm',family='binomial',trControl=ctrl)
#model 3
model3 <- train(rotten ~ 1 + content_rating + year + imbdRating + numVotes + drama + action + documentary,
    data=movie_cl,method='glm',family='binomial',trControl=ctrl)
#model 4
model4 <- train(rotten ~ 1 + content_rating + lifetime_gross + imbdRating + numVotes + drama + action + documentary + horror,
    data=movie_cl,method='glm',family='binomial',trControl=ctrl)
#model 5
model5 <- train(rotten ~ 1 + content_rating + imbdRating + numVotes + drama + action + documentary + horror,
    data=movie_cl,method='glm',family='binomial',trControl=ctrl)
```

```{r}
rbind(model1$results,model2$results,model3$results,model4$results,model5$results)
```

From the table we can see that the 2nd model has the most testing accuracy and largest Cohen's kappa value. The std dev of these values are also quite less, giving us confidence in the models. 
The Cohen's kappa value of about 0.5 tells us that the model has moderate agreement. 

Let us now look at the AUC of our best model. 

- I could not figure out how to plot AUC with repeated CV, so doing it with a train-test split

```{r}
bestmodel <- train(rotten ~ 1 + content_rating + year + imbdRating + numVotes + drama + action + documentary,
                       data=movie_cl,method='glm',family='binomial')
```
```{r}
#xkabledply(bestmodel$finalModel, title = paste("Logistic Regression :", format(formula(bestmodel$finalModel)) )) #could not use ezids
summary(bestmodel)
```

```{r}
expcoeff = exp(bestmodel$finalModel$coefficients)
xkabledply( as.table(expcoeff), title = "Exponential of coefficients in Logit Reg" )
xkabledply( confint.default(bestmodel$finalModel), title = "CIs using standard errors" )
```
```{r}
library(regclass)
xkabledply( confusion_matrix(bestmodel), title = "Confusion matrix from Logit Model" )
```
```{r}
admitNullLogit <- glm(rotten ~ 1, data = movie_cl, family = "binomial")
mcFadden = 1 - logLik(bestmodel$finalModel)/logLik(admitNullLogit)
mcFadden
```

```{r}
library(pROC)
sample <- sample(c(TRUE, FALSE), nrow(movie_cl), replace=TRUE, prob=c(0.7,0.3))
train <- movie_cl[sample, ]
test <- movie_cl[!sample, ]
model = glm(rotten ~ 1 + content_rating + year + imbdRating + numVotes + drama + action + documentary,
            data = movie_cl, family="binomial")
pred <- predict(model,test,type="response")
roc = roc(test$rotten~pred,plot=TRUE,print.auc=TRUE)
#auc(test$rotten,pred)
```

## Conclusion
SMART Questions:
1. What features affect IMDb Rating?
    We found that the features- runtime, tomatometer count, IMDb votes,  tomatometer rating, and the genres drama,             comedy, horror, and biography, affected the IMDb Rating.
    
2. What features affect can be used to predict movie genre?
  Drama genre - content rating (PG-13 and R), runtime, tomatometer rating, lifetime gross, year, IMDb Rating, IMDb votes
  Comedy genre - content rating (NR and R) , runtime, tomatometer rating, lifetime gross, year, IMDb Rating
  Romance genre - content rating (NR, PG-13, and R) , runtime, tomatometer rating, lifetime gross, year, IMDb votes

3. What features affect the Rotten Tomatoes freshness ratings?
  We found that the important features are - IMDb rating, content rating, IMDb votes, year and genres- documentary,        drama, action.

While we had relative success with our linear model to predict IMDb rating and using logistic regression to predict tomatometer status, the models for predicting the various genres were not fitted super well.

## Limitations and Extensions

Because both the rotten tomatoes and IMDb datasets have a genre variable, we as a group had to guess which one was better. Since the IMDb dataset overall is more complete and cleaner, we chose to use that one. They do generate slightly different counts, however both of the top two genres are the same across datasets. The rotten tomatoes data gives action as the third most common movie type. We have no way to analyze which of these two variables actually classifies the movies more accurately, especially given that each film can have one to three genres listed. We also didn't account for the difference between a movie which lists the first genre as drama versus those which listed it second of third. 

In the future, classifying genre could be treated as a classification problem, which may yield more significant results. For instance, we could use multinomial logistic regression, a neural network, or a random forest to classify all the genres, rather than analyzing each separately. Another investigation we would consider is a deeper look into seasonal trends and overarching trends in time for box office and IMDb rating.



